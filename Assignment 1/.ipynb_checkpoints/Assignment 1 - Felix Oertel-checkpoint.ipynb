{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Sheet 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit = pd.read_table(\"fruit_data_with_colors.txt\")\n",
    "fruit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: k nearest neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate data into features and class\n",
    "X = fruit.loc[:,\"mass\":\"color_score\"].values\n",
    "y = fruit.loc[:,\"fruit_name\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training and testing sets (using a fixed seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1234)\n",
    "#Remove mean of the training and testing data and scale to unit variance. \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the confusion matrix and the accuracy score using the k-closest-neighbour classifier for different values of k\n",
    "for k in range(1, 9):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k)  \n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"kNN (k=\"+str(k)+\")\"+\" %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Library Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the confusion matrix and test score using the library gaussian naive bayes classifier.\n",
    "classifier2 = GaussianNB()  \n",
    "classifier2.fit(X_train, y_train)\n",
    "y_pred = classifier2.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"BN %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Hand-coded Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBGaussian(object):\n",
    "    ''' \n",
    "    A class to implement the Gaussian Naive Bayes classifier. \n",
    "    Assuming we have n features in each feature vector and m feature vectors available for training.\n",
    "    \n",
    "    Inputs:\n",
    "    X - (m x n) - ndarray containing the feature vectors, with X[i, :] being the i-th feature vector.\n",
    "    y - (1 x n) - ndarray containing the class labels corresponding to each feature vector in X\n",
    "    \n",
    "    Methods:\n",
    "    train - calculates statistics (i.e. mean and standard variance) for each class using feature vector matrix X\n",
    "    predict - taking some testing data X_test, estimates corresponding class by using highest likelihood estimation \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, X, y):\n",
    "        #Initialise the object by calculating the statistics of the input data and its unique class labels.\n",
    "        self.train(X, y)\n",
    "        self.total_rows = len(X)\n",
    "        self.labels = np.unique(y)\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        #Train the classifier giving some input feature vector matrix X and class vector y\n",
    "        self.dataset = self.__split_data_class__(X, y)\n",
    "        self.stats = self.calc_statistics()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        #given a test data set that has the same amount of features as the training data, predict class by using maximum likelihood estimation\n",
    "        y_predict = []\n",
    "        for row in X:\n",
    "            #calculate cond. probabilities p(class y_j | X_i) (i.e. probability that feature vector X_i corresponds to class y_j )\n",
    "            probabilities = self.calc_probabilities(row)\n",
    "            #take the index, where the cond. probability is maximal and append label corresponding to that index to the results\n",
    "            index = np.argmax([probabilities[label] for label in self.labels])\n",
    "            y_predict.append(self.labels[index])\n",
    "        return y_predict\n",
    "        \n",
    "    def __split_data_class__(self, X, y):\n",
    "        #create a dictionary. Use labels as keys and arrays of corresponding feature vectors as values.\n",
    "        dataset = dict()\n",
    "        for label in y:\n",
    "            dataset[label] = X[y==label]\n",
    "        return dataset\n",
    "    \n",
    "    def mean(self, data):\n",
    "        #calculate the mean of a 1-d array\n",
    "        return np.sum(data)/len(data)\n",
    "    \n",
    "    def stddev(self, data):\n",
    "        #calculate the standard deviation of a 1-d array\n",
    "        mean = self.mean(data)\n",
    "        return np.sqrt(np.sum((data-mean)**2)/(len(data)-1))\n",
    "    \n",
    "    def normal_dist(self, x, mean, stddev):\n",
    "        #calculate the normal distribution at a point x, given the mean and standard deviation of the distribution\n",
    "        y = np.exp(-(x-mean)**2/(2*stddev**2))\n",
    "        return 1/np.sqrt(2*np.pi)/stddev*y\n",
    "    \n",
    "    def calc_statistics(self):\n",
    "        #calculate the statistics (i.e. mean and std. dev.) for given training data and save them inside a dictionary.\n",
    "        data = self.dataset\n",
    "        stats = dict()\n",
    "        for label in data:\n",
    "            #calculate the statistics for the data corresponding to a given label and save them as a tuple. \n",
    "            #use the label as the dictionary key\n",
    "            stats[label]=[(self.mean(data[label][:,i]), self.stddev(data[label][:,i])) for i in range(len(data[label][0]))]\n",
    "        return stats\n",
    "    \n",
    "    def calc_probabilities(self, x):\n",
    "        #calculate the conditionaly probabilities that test feature vector x corresponds to the labels.\n",
    "        #returns a dictionary, where the class labels are the keys and the values are the corresponding probabilities.\n",
    "        data = self.dataset\n",
    "        stats = self.stats\n",
    "        total_rows = self.total_rows\n",
    "        probabilities=dict()\n",
    "        for label in data:\n",
    "            #exclude label that has no corresponding feature vectors\n",
    "            if len(data[label])==0:\n",
    "                probabilities=-np.inf\n",
    "                continue\n",
    "            #calculate log probability p(yj) of class yj by calculating its frequency inside the training data\n",
    "            probabilities[label] = np.log(len(data[label])/float(total_rows))\n",
    "            for i in range(len(stats[label])):\n",
    "                #for each label yj and each feature xi, calculate p(xi|yj)\n",
    "                #this is done by evaluating normal dist. corresponding to label yj at point xi\n",
    "                mean, stddev = stats[label][i]\n",
    "                #if p(xi|yj) is too small, set log probability to -inf, which corresponds to p(xi|yj) = 0\n",
    "                if self.normal_dist(x[i], mean, stddev)<10**(-50):\n",
    "                    probabilities[label] = -np.inf\n",
    "                    break\n",
    "                #add all log probabilities of each feature together\n",
    "                probabilities[label] += np.log(self.normal_dist(x[i], mean, stddev))\n",
    "        return probabilities\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = NBGaussian(X_train, y_train)\n",
    "y_predict=classifier3.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"BN %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
